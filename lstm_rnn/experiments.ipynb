{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f24ec7",
   "metadata": {},
   "source": [
    "#### Project Description: Next Word Prediction Using LSTM\n",
    "\n",
    "This project aims to develop a deep learning model for predicting the next word in a given sequence of words. The model is built using Long Short-Term Memory (LSTM) networks, which are well-suited for sequence prediction tasks. The project includes the following steps:\n",
    "\n",
    "1- Data Collection: We use the text of Shakespeare's \"Hamlet\" as our dataset. This rich, complex text provides a good challenge for our model.\n",
    "\n",
    "2- Data Preprocessing: The text data is tokenized, converted into sequences, and padded to ensure uniform input lengths. The sequences are then split into training and testing sets.\n",
    "\n",
    "3- Model Building: An LSTM model is constructed with an embedding layer, two LSTM layers, and a dense output layer with a softmax activation function to predict the probability of the next word.\n",
    "\n",
    "4- Model Training: The model is trained using the prepared sequences, with early stopping implemented to prevent overfitting. Early stopping monitors the validation loss and stops training when the loss stops improving.\n",
    "\n",
    "5- Model Evaluation: The model is evaluated using a set of example sentences to test its ability to predict the next word accurately.\n",
    "\n",
    "6- Deployment: A Streamlit web application is developed to allow users to input a sequence of words and get the predicted next word in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d57460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "805fbd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\indra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data collection \n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "import pandas as pd \n",
    "\n",
    "# Load the dataset \n",
    "data = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "# Save to file \n",
    "with open('hamlet.txt', 'w') as file:\n",
    "    file.write(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f28fe28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Code\\Python\\ann-classification\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing \n",
    "import numpy as np \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset \n",
    "with open('hamlet.txt', 'r') as file:\n",
    "    text = file.read().lower() \n",
    "\n",
    "# Tokenize the text - creating indexes for words \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "# tokenizer.word_index\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3dc56d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input sequences \n",
    "input_seq = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    # print(token_list)\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_seq.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44f10b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences \n",
    "max_sequence_len = max([len(x) for x in input_seq])\n",
    "input_sequences = np.array(pad_sequences(input_seq, maxlen=max_sequence_len, padding = 'pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74859614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Create predictors and label \n",
    "import tensorflow as tf \n",
    "# input_sequences[:, :-1] → takes all rows and all columns except the last\n",
    "# input_sequences[:, -1] → takes all rows and only the last column \n",
    "x, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "\n",
    "# transforms each label into a vector of length = total_words \n",
    "# 1 is at the index of the label and all others are 0\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d42bb2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8bdaf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Code\\Python\\ann-classification\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Code\\Python\\ann-classification\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 13, 100)           481800    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 13, 150)           150600    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 13, 150)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               100400    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4818)              486618    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1219418 (4.65 MB)\n",
      "Trainable params: 1219418 (4.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Train our LSTM RNN\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Define the model \n",
    "model = Sequential()\n",
    "# Turns word indexes into vectors\n",
    "# total_words is vocabulary size, each word mappe to 100 dimension vector\n",
    "model.add(Embedding(total_words, 100, input_length = max_sequence_len-1))\n",
    "# First LSTM layer, returns full sequence of outputs\n",
    "model.add(LSTM(150, return_sequences = True))\n",
    "# randomly dropout 20% neurons during training to prevent overfitting\n",
    "model.add(Dropout(0.2))\n",
    "# returns the final hidden state - used for prediction\n",
    "model.add(LSTM(100))\n",
    "# Output layer - softmax turns output into probabilities\n",
    "model.add(Dense(total_words, activation = 'softmax'))\n",
    "\n",
    "# Compile the model \n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6f5cb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From d:\\Code\\Python\\ann-classification\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Code\\Python\\ann-classification\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "644/644 [==============================] - 23s 24ms/step - loss: 6.8917 - accuracy: 0.0325 - val_loss: 6.7758 - val_accuracy: 0.0367\n",
      "Epoch 2/50\n",
      "644/644 [==============================] - 16s 24ms/step - loss: 6.4600 - accuracy: 0.0384 - val_loss: 6.8644 - val_accuracy: 0.0443\n",
      "Epoch 3/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 6.3213 - accuracy: 0.0453 - val_loss: 6.8739 - val_accuracy: 0.0505\n",
      "Epoch 4/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 6.1876 - accuracy: 0.0511 - val_loss: 6.9202 - val_accuracy: 0.0534\n",
      "Epoch 5/50\n",
      "644/644 [==============================] - 14s 21ms/step - loss: 6.0532 - accuracy: 0.0555 - val_loss: 6.9518 - val_accuracy: 0.0579\n",
      "Epoch 6/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 5.9163 - accuracy: 0.0625 - val_loss: 7.0300 - val_accuracy: 0.0583\n",
      "Epoch 7/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 5.7842 - accuracy: 0.0683 - val_loss: 7.0586 - val_accuracy: 0.0620\n",
      "Epoch 8/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 5.6493 - accuracy: 0.0755 - val_loss: 7.0992 - val_accuracy: 0.0626\n",
      "Epoch 9/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 5.5120 - accuracy: 0.0857 - val_loss: 7.1740 - val_accuracy: 0.0635\n",
      "Epoch 10/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 5.3835 - accuracy: 0.0887 - val_loss: 7.2588 - val_accuracy: 0.0680\n",
      "Epoch 11/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 5.2622 - accuracy: 0.0965 - val_loss: 7.3468 - val_accuracy: 0.0666\n",
      "Epoch 12/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 5.1444 - accuracy: 0.1034 - val_loss: 7.4302 - val_accuracy: 0.0663\n",
      "Epoch 13/50\n",
      "644/644 [==============================] - 12s 19ms/step - loss: 5.0298 - accuracy: 0.1078 - val_loss: 7.5424 - val_accuracy: 0.0670\n",
      "Epoch 14/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 4.9165 - accuracy: 0.1134 - val_loss: 7.6522 - val_accuracy: 0.0692\n",
      "Epoch 15/50\n",
      "644/644 [==============================] - 12s 19ms/step - loss: 4.8053 - accuracy: 0.1187 - val_loss: 7.7867 - val_accuracy: 0.0692\n",
      "Epoch 16/50\n",
      "644/644 [==============================] - 12s 19ms/step - loss: 4.6986 - accuracy: 0.1243 - val_loss: 7.8727 - val_accuracy: 0.0666\n",
      "Epoch 17/50\n",
      "644/644 [==============================] - 12s 19ms/step - loss: 4.5962 - accuracy: 0.1266 - val_loss: 7.9819 - val_accuracy: 0.0661\n",
      "Epoch 18/50\n",
      "644/644 [==============================] - 12s 18ms/step - loss: 4.4982 - accuracy: 0.1348 - val_loss: 8.1566 - val_accuracy: 0.0688\n",
      "Epoch 19/50\n",
      "644/644 [==============================] - 12s 19ms/step - loss: 4.3973 - accuracy: 0.1415 - val_loss: 8.2951 - val_accuracy: 0.0661\n",
      "Epoch 20/50\n",
      "644/644 [==============================] - 14s 21ms/step - loss: 4.3054 - accuracy: 0.1486 - val_loss: 8.4381 - val_accuracy: 0.0663\n",
      "Epoch 21/50\n",
      "644/644 [==============================] - 12s 19ms/step - loss: 4.2130 - accuracy: 0.1574 - val_loss: 8.5392 - val_accuracy: 0.0657\n",
      "Epoch 22/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 4.1295 - accuracy: 0.1688 - val_loss: 8.7067 - val_accuracy: 0.0641\n",
      "Epoch 23/50\n",
      "644/644 [==============================] - 12s 19ms/step - loss: 4.0445 - accuracy: 0.1782 - val_loss: 8.8371 - val_accuracy: 0.0663\n",
      "Epoch 24/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 3.9629 - accuracy: 0.1906 - val_loss: 8.9405 - val_accuracy: 0.0645\n",
      "Epoch 25/50\n",
      "644/644 [==============================] - 12s 19ms/step - loss: 3.8830 - accuracy: 0.2026 - val_loss: 9.1095 - val_accuracy: 0.0618\n",
      "Epoch 26/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 3.8131 - accuracy: 0.2141 - val_loss: 9.2229 - val_accuracy: 0.0610\n",
      "Epoch 27/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 3.7410 - accuracy: 0.2259 - val_loss: 9.3307 - val_accuracy: 0.0616\n",
      "Epoch 28/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 3.6741 - accuracy: 0.2402 - val_loss: 9.4663 - val_accuracy: 0.0628\n",
      "Epoch 29/50\n",
      "644/644 [==============================] - 13s 21ms/step - loss: 3.6125 - accuracy: 0.2466 - val_loss: 9.5518 - val_accuracy: 0.0614\n",
      "Epoch 30/50\n",
      "644/644 [==============================] - 12s 19ms/step - loss: 3.5457 - accuracy: 0.2594 - val_loss: 9.6437 - val_accuracy: 0.0612\n",
      "Epoch 31/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 3.4921 - accuracy: 0.2692 - val_loss: 9.7709 - val_accuracy: 0.0624\n",
      "Epoch 32/50\n",
      "644/644 [==============================] - 15s 23ms/step - loss: 3.4359 - accuracy: 0.2751 - val_loss: 9.8801 - val_accuracy: 0.0618\n",
      "Epoch 33/50\n",
      "644/644 [==============================] - 14s 21ms/step - loss: 3.3811 - accuracy: 0.2849 - val_loss: 9.9411 - val_accuracy: 0.0608\n",
      "Epoch 34/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 3.3268 - accuracy: 0.2951 - val_loss: 10.0564 - val_accuracy: 0.0600\n",
      "Epoch 35/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 3.2728 - accuracy: 0.3049 - val_loss: 10.1494 - val_accuracy: 0.0624\n",
      "Epoch 36/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 3.2251 - accuracy: 0.3131 - val_loss: 10.2235 - val_accuracy: 0.0567\n",
      "Epoch 37/50\n",
      "644/644 [==============================] - 13s 19ms/step - loss: 3.1721 - accuracy: 0.3241 - val_loss: 10.3283 - val_accuracy: 0.0587\n",
      "Epoch 38/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 3.1310 - accuracy: 0.3305 - val_loss: 10.3993 - val_accuracy: 0.0573\n",
      "Epoch 39/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 3.0889 - accuracy: 0.3372 - val_loss: 10.5221 - val_accuracy: 0.0604\n",
      "Epoch 40/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 3.0423 - accuracy: 0.3477 - val_loss: 10.5786 - val_accuracy: 0.0579\n",
      "Epoch 41/50\n",
      "644/644 [==============================] - 12s 19ms/step - loss: 2.9977 - accuracy: 0.3525 - val_loss: 10.6868 - val_accuracy: 0.0565\n",
      "Epoch 42/50\n",
      "644/644 [==============================] - 12s 19ms/step - loss: 2.9575 - accuracy: 0.3620 - val_loss: 10.7178 - val_accuracy: 0.0561\n",
      "Epoch 43/50\n",
      "644/644 [==============================] - 12s 19ms/step - loss: 2.9218 - accuracy: 0.3658 - val_loss: 10.8100 - val_accuracy: 0.0567\n",
      "Epoch 44/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 2.8870 - accuracy: 0.3738 - val_loss: 10.8866 - val_accuracy: 0.0604\n",
      "Epoch 45/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 2.8415 - accuracy: 0.3850 - val_loss: 10.9285 - val_accuracy: 0.0548\n",
      "Epoch 46/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 2.8075 - accuracy: 0.3880 - val_loss: 11.0302 - val_accuracy: 0.0548\n",
      "Epoch 47/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 2.7680 - accuracy: 0.3961 - val_loss: 11.1059 - val_accuracy: 0.0561\n",
      "Epoch 48/50\n",
      "644/644 [==============================] - 13s 21ms/step - loss: 2.7344 - accuracy: 0.4031 - val_loss: 11.1943 - val_accuracy: 0.0528\n",
      "Epoch 49/50\n",
      "644/644 [==============================] - 13s 20ms/step - loss: 2.7017 - accuracy: 0.4133 - val_loss: 11.2703 - val_accuracy: 0.0536\n",
      "Epoch 50/50\n",
      "644/644 [==============================] - 14s 21ms/step - loss: 2.6661 - accuracy: 0.4176 - val_loss: 11.3360 - val_accuracy: 0.0521\n"
     ]
    }
   ],
   "source": [
    "# Compile the model \n",
    "history = model.fit(x_train, y_train, epochs = 50, validation_data=(x_test, y_test), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2358994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import predict_next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98af32be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 13)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pad_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(model.input_shape)\n\u001b[32m      3\u001b[39m max_sequence_length = model.input_shape[\u001b[32m1\u001b[39m] + \u001b[32m1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m next_word = \u001b[43mpredict_next_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_sequence_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNext word prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnext_word\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code\\Python\\ann-classification\\lstm_rnn\\helper.py:7\u001b[39m, in \u001b[36mpredict_next_word\u001b[39m\u001b[34m(model, tokenizer, text, max_sequence_length)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token_list) >= max_sequence_length:\n\u001b[32m      6\u001b[39m     token_list = token_list[-(max_sequence_length-\u001b[32m1\u001b[39m):]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m token_list = \u001b[43mpad_sequences\u001b[49m([token_list], maxlen=max_sequence_length-\u001b[32m1\u001b[39m, padding=\u001b[33m'\u001b[39m\u001b[33mpre\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m predicted = model.predict(token_list, verbose=\u001b[32m0\u001b[39m)\n\u001b[32m      9\u001b[39m predicted_word_index = np.argmax(predicted, axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'pad_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "input_text = \"Is not this\"\n",
    "print(model.input_shape)\n",
    "max_sequence_length = model.input_shape[1] + 1\n",
    "next_word = predict_next_word(model, tokenizer, input_text, max_sequence_length)\n",
    "print(f\"Next word prediction: {next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7212c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model \n",
    "model.save('next_word_lstm_model.h5')\n",
    "import pickle \n",
    "with  open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
